{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>262.000000</td>\n",
       "      <td>267.899994</td>\n",
       "      <td>250.029999</td>\n",
       "      <td>254.259995</td>\n",
       "      <td>254.259995</td>\n",
       "      <td>11896100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-06</th>\n",
       "      <td>247.699997</td>\n",
       "      <td>266.700012</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>265.720001</td>\n",
       "      <td>265.720001</td>\n",
       "      <td>12595800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-07</th>\n",
       "      <td>266.579987</td>\n",
       "      <td>272.450012</td>\n",
       "      <td>264.329987</td>\n",
       "      <td>264.559998</td>\n",
       "      <td>264.559998</td>\n",
       "      <td>8981500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-08</th>\n",
       "      <td>267.079987</td>\n",
       "      <td>267.619995</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.100006</td>\n",
       "      <td>250.100006</td>\n",
       "      <td>9306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-09</th>\n",
       "      <td>253.850006</td>\n",
       "      <td>255.800003</td>\n",
       "      <td>236.110001</td>\n",
       "      <td>249.470001</td>\n",
       "      <td>249.470001</td>\n",
       "      <td>16906900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-31</th>\n",
       "      <td>401.970001</td>\n",
       "      <td>427.700012</td>\n",
       "      <td>398.200012</td>\n",
       "      <td>427.140015</td>\n",
       "      <td>427.140015</td>\n",
       "      <td>20047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-01</th>\n",
       "      <td>432.959991</td>\n",
       "      <td>458.480011</td>\n",
       "      <td>425.540009</td>\n",
       "      <td>457.130005</td>\n",
       "      <td>457.130005</td>\n",
       "      <td>22542300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-02</th>\n",
       "      <td>448.250000</td>\n",
       "      <td>451.980011</td>\n",
       "      <td>426.480011</td>\n",
       "      <td>429.480011</td>\n",
       "      <td>429.480011</td>\n",
       "      <td>14346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-03</th>\n",
       "      <td>421.440002</td>\n",
       "      <td>429.260010</td>\n",
       "      <td>404.279999</td>\n",
       "      <td>405.600006</td>\n",
       "      <td>405.600006</td>\n",
       "      <td>9905200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-04</th>\n",
       "      <td>407.309998</td>\n",
       "      <td>412.769989</td>\n",
       "      <td>396.640015</td>\n",
       "      <td>410.170013</td>\n",
       "      <td>410.170013</td>\n",
       "      <td>7782400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1009 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2018-02-05  262.000000  267.899994  250.029999  254.259995  254.259995   \n",
       "2018-02-06  247.699997  266.700012  245.000000  265.720001  265.720001   \n",
       "2018-02-07  266.579987  272.450012  264.329987  264.559998  264.559998   \n",
       "2018-02-08  267.079987  267.619995  250.000000  250.100006  250.100006   \n",
       "2018-02-09  253.850006  255.800003  236.110001  249.470001  249.470001   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2022-01-31  401.970001  427.700012  398.200012  427.140015  427.140015   \n",
       "2022-02-01  432.959991  458.480011  425.540009  457.130005  457.130005   \n",
       "2022-02-02  448.250000  451.980011  426.480011  429.480011  429.480011   \n",
       "2022-02-03  421.440002  429.260010  404.279999  405.600006  405.600006   \n",
       "2022-02-04  407.309998  412.769989  396.640015  410.170013  410.170013   \n",
       "\n",
       "              Volume  \n",
       "Date                  \n",
       "2018-02-05  11896100  \n",
       "2018-02-06  12595800  \n",
       "2018-02-07   8981500  \n",
       "2018-02-08   9306700  \n",
       "2018-02-09  16906900  \n",
       "...              ...  \n",
       "2022-01-31  20047500  \n",
       "2022-02-01  22542300  \n",
       "2022-02-02  14346000  \n",
       "2022-02-03   9905200  \n",
       "2022-02-04   7782400  \n",
       "\n",
       "[1009 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import time as tm\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data preparation\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import deque\n",
    "\n",
    "# AI\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "\n",
    "# Graphics library\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# SETTINGS\n",
    "\n",
    "# Window size or the sequence length, 7 (1 week)\n",
    "N_STEPS = 7\n",
    "\n",
    "# Lookup steps, 1 is the next day, 3 = after tomorrow\n",
    "LOOKUP_STEPS = [i for i in range(1, 51)]\n",
    "# LOOKUP_STEPS = [1,2,3]\n",
    "\n",
    "df = pd.read_csv('NFLX.csv')\n",
    "df.set_index('Date', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>254.259995</td>\n",
       "      <td>2018-02-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-06</th>\n",
       "      <td>265.720001</td>\n",
       "      <td>2018-02-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-07</th>\n",
       "      <td>264.559998</td>\n",
       "      <td>2018-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-08</th>\n",
       "      <td>250.100006</td>\n",
       "      <td>2018-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-09</th>\n",
       "      <td>249.470001</td>\n",
       "      <td>2018-02-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-31</th>\n",
       "      <td>427.140015</td>\n",
       "      <td>2022-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-01</th>\n",
       "      <td>457.130005</td>\n",
       "      <td>2022-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-02</th>\n",
       "      <td>429.480011</td>\n",
       "      <td>2022-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-03</th>\n",
       "      <td>405.600006</td>\n",
       "      <td>2022-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-04</th>\n",
       "      <td>410.170013</td>\n",
       "      <td>2022-02-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1009 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Close        date\n",
       "Date                              \n",
       "2018-02-05  254.259995  2018-02-05\n",
       "2018-02-06  265.720001  2018-02-06\n",
       "2018-02-07  264.559998  2018-02-07\n",
       "2018-02-08  250.100006  2018-02-08\n",
       "2018-02-09  249.470001  2018-02-09\n",
       "...                ...         ...\n",
       "2022-01-31  427.140015  2022-01-31\n",
       "2022-02-01  457.130005  2022-02-01\n",
       "2022-02-02  429.480011  2022-02-02\n",
       "2022-02-03  405.600006  2022-02-03\n",
       "2022-02-04  410.170013  2022-02-04\n",
       "\n",
       "[1009 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_df = df.drop(['Open', 'High', 'Low','Adj Close','Volume'], axis=1)\n",
    "init_df['date'] = init_df.index\n",
    "\n",
    "init_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>0.044516</td>\n",
       "      <td>2018-02-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-06</th>\n",
       "      <td>0.069548</td>\n",
       "      <td>2018-02-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-07</th>\n",
       "      <td>0.067015</td>\n",
       "      <td>2018-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-08</th>\n",
       "      <td>0.035430</td>\n",
       "      <td>2018-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-09</th>\n",
       "      <td>0.034053</td>\n",
       "      <td>2018-02-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-31</th>\n",
       "      <td>0.422140</td>\n",
       "      <td>2022-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-01</th>\n",
       "      <td>0.487648</td>\n",
       "      <td>2022-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-02</th>\n",
       "      <td>0.427251</td>\n",
       "      <td>2022-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-03</th>\n",
       "      <td>0.375090</td>\n",
       "      <td>2022-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-04</th>\n",
       "      <td>0.385072</td>\n",
       "      <td>2022-02-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1009 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Close        date\n",
       "Date                            \n",
       "2018-02-05  0.044516  2018-02-05\n",
       "2018-02-06  0.069548  2018-02-06\n",
       "2018-02-07  0.067015  2018-02-07\n",
       "2018-02-08  0.035430  2018-02-08\n",
       "2018-02-09  0.034053  2018-02-09\n",
       "...              ...         ...\n",
       "2022-01-31  0.422140  2022-01-31\n",
       "2022-02-01  0.487648  2022-02-01\n",
       "2022-02-02  0.427251  2022-02-02\n",
       "2022-02-03  0.375090  2022-02-03\n",
       "2022-02-04  0.385072  2022-02-04\n",
       "\n",
       "[1009 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "init_df['Close'] = scaler.fit_transform(np.expand_dims(init_df['Close'].values, axis=1))\n",
    "\n",
    "init_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareData(days):\n",
    "  df = init_df.copy()\n",
    "  df['future'] = df['Close'].shift(-days)\n",
    "  last_sequence = np.array(df[['Close']].tail(days))\n",
    "  df.dropna(inplace=True)\n",
    "  sequence_data = []\n",
    "  sequences = deque(maxlen=N_STEPS)\n",
    "\n",
    "  for entry, target in zip(df[['Close'] + ['date']].values, df['future'].values):\n",
    "      sequences.append(entry)\n",
    "      if len(sequences) == N_STEPS:\n",
    "          sequence_data.append([np.array(sequences), target])\n",
    "\n",
    "  last_sequence = list([s[:len(['Close'])] for s in sequences]) + list(last_sequence)\n",
    "  last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "\n",
    "  # construct the X's and Y's\n",
    "  X, Y = [], []\n",
    "  for seq, target in sequence_data:\n",
    "      X.append(seq)\n",
    "      Y.append(target)\n",
    "\n",
    "  # convert to numpy arrays\n",
    "  X = np.array(X)\n",
    "  Y = np.array(Y)\n",
    "\n",
    "  return df, last_sequence, X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTrainedModel(x_train, y_train):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(60, return_sequences=True, input_shape=(N_STEPS, len(['Close']))))\n",
    "  model.add(Dropout(0.3))\n",
    "  model.add(LSTM(120, return_sequences=False))\n",
    "  model.add(Dropout(0.3))\n",
    "  model.add(Dense(20))\n",
    "  model.add(Dense(1))\n",
    "\n",
    "  BATCH_SIZE = 8\n",
    "  EPOCHS = 20\n",
    "\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "  model.fit(x_train, y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            verbose=1)\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "126/126 [==============================] - 3s 5ms/step - loss: 0.0094\n",
      "Epoch 2/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0049\n",
      "Epoch 3/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0030\n",
      "Epoch 4/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0033\n",
      "Epoch 5/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0029\n",
      "Epoch 6/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0027\n",
      "Epoch 7/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0022\n",
      "Epoch 8/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0024\n",
      "Epoch 9/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0020\n",
      "Epoch 10/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0022\n",
      "Epoch 11/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0018\n",
      "Epoch 12/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0019\n",
      "Epoch 13/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0019\n",
      "Epoch 14/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0020\n",
      "Epoch 15/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0015\n",
      "Epoch 16/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0017\n",
      "Epoch 17/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0017\n",
      "Epoch 18/20\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.0017\n",
      "Epoch 19/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0017\n",
      "Epoch 20/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0017\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 7, 60)             14880     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 7, 60)             0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 120)               86880     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 120)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                2420      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,201\n",
      "Trainable params: 104,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 380ms/step\n",
      "Epoch 1/20\n",
      "126/126 [==============================] - 2s 4ms/step - loss: 0.0116\n",
      "Epoch 2/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0042\n",
      "Epoch 3/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0038\n",
      "Epoch 4/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0038\n",
      "Epoch 5/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0039\n",
      "Epoch 6/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0032\n",
      "Epoch 7/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0031\n",
      "Epoch 8/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0029\n",
      "Epoch 9/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0027\n",
      "Epoch 10/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0027\n",
      "Epoch 11/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0025\n",
      "Epoch 12/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0025\n",
      "Epoch 13/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0024\n",
      "Epoch 14/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0026\n",
      "Epoch 15/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0024\n",
      "Epoch 16/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0024\n",
      "Epoch 17/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0023\n",
      "Epoch 18/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0023\n",
      "Epoch 19/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0024\n",
      "Epoch 20/20\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 0.0021\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 7, 60)             14880     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 7, 60)             0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 120)               86880     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 120)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                2420      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,201\n",
      "Trainable params: 104,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 368ms/step\n",
      "Epoch 1/20\n",
      "125/125 [==============================] - 2s 3ms/step - loss: 0.0107\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0047\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0045\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0040\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0050\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0038\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0038\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0034\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0032\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0031\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0030\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0032\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0028\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0028\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 7, 60)             14880     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 7, 60)             0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 120)               86880     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 120)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 20)                2420      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,201\n",
      "Trainable params: 104,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "Epoch 1/20\n",
      "125/125 [==============================] - 2s 5ms/step - loss: 0.0141\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0052\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0050\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0051\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0050\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0046\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0047\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0042\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0040\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0037\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0034\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0036\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0038\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0035\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0036\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0039\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0037\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0035\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0036\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0033\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 7, 60)             14880     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 7, 60)             0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 120)               86880     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 120)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 20)                2420      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,201\n",
      "Trainable params: 104,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 360ms/step\n",
      "Epoch 1/20\n",
      "125/125 [==============================] - 2s 4ms/step - loss: 0.0141\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0058\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0058\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0055\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0047\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0051\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0046\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0048\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0045\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0047\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0041\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0046\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0044\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0043\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0042\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0045\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0045\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0039\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0040\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0043\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 7, 60)             14880     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 7, 60)             0         \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 120)               86880     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 120)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 20)                2420      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,201\n",
      "Trainable params: 104,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002810CB77060> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "Epoch 1/20\n",
      "125/125 [==============================] - 3s 5ms/step - loss: 0.0129\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0067\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0062\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0070\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0055\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0059\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0048\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0050\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0047\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0050\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0047\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0046\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0045\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0047\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0044\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0044\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0047\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0047\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0046\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0047\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_10 (LSTM)              (None, 7, 60)             14880     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 7, 60)             0         \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 120)               86880     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 20)                2420      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,201\n",
      "Trainable params: 104,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002817AAAC540> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 360ms/step\n",
      "Epoch 1/20\n",
      "125/125 [==============================] - 2s 4ms/step - loss: 0.0147\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0067\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0058\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0057\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0063\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0061\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0056\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0058\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0054\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0054\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0054\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0052\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0051\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0049\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0052\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0048\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0050\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0052\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0048\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0052\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 7, 60)             14880     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 7, 60)             0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 120)               86880     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 20)                2420      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,201\n",
      "Trainable params: 104,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 365ms/step\n",
      "Epoch 1/20\n",
      "125/125 [==============================] - 2s 4ms/step - loss: 0.0155\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0073\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0076\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0064\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0065\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0059\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0062\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0061\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0057\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0059\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0056\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0056\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0054\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0054\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0055\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0054\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0055\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0055\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0056\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0054\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_14 (LSTM)              (None, 7, 60)             14880     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 7, 60)             0         \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 120)               86880     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 20)                2420      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,201\n",
      "Trainable params: 104,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 368ms/step\n",
      "Epoch 1/20\n",
      "125/125 [==============================] - 3s 5ms/step - loss: 0.0139\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0080\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0076\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0069\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0067\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0069\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0063\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0064\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0061\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0062\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0059\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0061\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0060\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0059\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0062\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0057\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0061\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0059\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0061\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0060\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_16 (LSTM)              (None, 7, 60)             14880     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 7, 60)             0         \n",
      "                                                                 \n",
      " lstm_17 (LSTM)              (None, 120)               86880     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 20)                2420      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,201\n",
      "Trainable params: 104,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 366ms/step\n",
      "Epoch 1/20\n",
      "125/125 [==============================] - 2s 5ms/step - loss: 0.0148\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0088\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0082\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0075\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0073\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0070\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0067\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0068\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0068\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0076\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0062\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0064\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0068\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0066\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0060\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0071\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0059\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0061\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0061\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0062\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_18 (LSTM)              (None, 7, 60)             14880     \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 7, 60)             0         \n",
      "                                                                 \n",
      " lstm_19 (LSTM)              (None, 120)               86880     \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 20)                2420      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,201\n",
      "Trainable params: 104,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "Epoch 1/20\n",
      "124/124 [==============================] - 2s 4ms/step - loss: 0.0148\n",
      "Epoch 2/20\n",
      "124/124 [==============================] - 0s 4ms/step - loss: 0.0083\n",
      "Epoch 3/20\n",
      "124/124 [==============================] - 0s 4ms/step - loss: 0.0081\n",
      "Epoch 4/20\n",
      "124/124 [==============================] - 0s 4ms/step - loss: 0.0083\n",
      "Epoch 5/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.0072\n",
      "Epoch 6/20\n",
      "124/124 [==============================] - 0s 4ms/step - loss: 0.0073\n",
      "Epoch 7/20\n",
      "113/124 [==========================>...] - ETA: 0s - loss: 0.0081"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mikol\\OneDrive\\Dokumenty\\university\\Machine learning\\netflix_stocks\\netflix_lstm_tensorflow_3.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df, last_sequence, x_train, y_train \u001b[39m=\u001b[39m PrepareData(step)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m x_train \u001b[39m=\u001b[39m x_train[:, :, :\u001b[39mlen\u001b[39m([\u001b[39m'\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m'\u001b[39m])]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m GetTrainedModel(x_train, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m last_sequence \u001b[39m=\u001b[39m last_sequence[\u001b[39m-\u001b[39mN_STEPS:]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m last_sequence \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(last_sequence, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\mikol\\OneDrive\\Dokumenty\\university\\Machine learning\\netflix_stocks\\netflix_lstm_tensorflow_3.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m EPOCHS \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m           batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m           epochs\u001b[39m=\u001b[39;49mEPOCHS,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m           verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py:1676\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1674\u001b[0m callbacks\u001b[39m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m   1675\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m-> 1676\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   1677\u001b[0m         \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m             epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m             _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m         ):\n\u001b[0;32m   1684\u001b[0m             callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\data_adapter.py:1375\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m original_spe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1376\u001b[0m can_run_full_execution \u001b[39m=\u001b[39m (\n\u001b[0;32m   1377\u001b[0m     original_spe \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1378\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m original_spe\n\u001b[0;32m   1380\u001b[0m )\n\u001b[0;32m   1382\u001b[0m \u001b[39mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:647\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    646\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 647\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    648\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    649\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:777\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    774\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_variable_op()\n\u001b[0;32m    775\u001b[0m \u001b[39m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[39m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[1;32m--> 777\u001b[0m \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39;49midentity(value)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\ops\\array_ops.py:302\u001b[0m, in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39minput\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgraph\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    299\u001b[0m   \u001b[39m# Make sure we get an input with handle data attached from resource\u001b[39;00m\n\u001b[0;32m    300\u001b[0m   \u001b[39m# variables. Variables have correct handle data when graph building.\u001b[39;00m\n\u001b[0;32m    301\u001b[0m   \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\u001b[39minput\u001b[39m)\n\u001b[1;32m--> 302\u001b[0m ret \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39;49midentity(\u001b[39minput\u001b[39;49m, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m    303\u001b[0m \u001b[39m# Propagate handle data for happier shape inference for resource variables.\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39minput\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_handle_data\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:4885\u001b[0m, in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   4883\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   4884\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 4885\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   4886\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mIdentity\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39minput\u001b[39;49m)\n\u001b[0;32m   4887\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   4888\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for step in LOOKUP_STEPS:\n",
    "  df, last_sequence, x_train, y_train = PrepareData(step)\n",
    "  x_train = x_train[:, :, :len(['Close'])].astype(np.float32)\n",
    "\n",
    "  model = GetTrainedModel(x_train, y_train)\n",
    "\n",
    "  last_sequence = last_sequence[-N_STEPS:]\n",
    "  last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "  prediction = model.predict(last_sequence)\n",
    "  predicted_price = scaler.inverse_transform(prediction)[0][0]\n",
    "\n",
    "  predictions.append(round(float(predicted_price), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " 1000,\n",
       " ...]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mikol\\OneDrive\\Dokumenty\\university\\Machine learning\\netflix_stocks\\netflix_lstm_tensorflow_3.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m date_tomorrow \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m2022-02-05\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m date_after_tomorrow \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m2022-02-06\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m copy_df\u001b[39m.\u001b[39;49mloc[date_now] \u001b[39m=\u001b[39m [predictions[\u001b[39m0\u001b[39m], \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdate_now\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m copy_df\u001b[39m.\u001b[39mloc[date_tomorrow] \u001b[39m=\u001b[39m [predictions[\u001b[39m1\u001b[39m], \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdate_tomorrow\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/OneDrive/Dokumenty/university/Machine%20learning/netflix_stocks/netflix_lstm_tensorflow_3.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m copy_df\u001b[39m.\u001b[39mloc[date_after_tomorrow] \u001b[39m=\u001b[39m [predictions[\u001b[39m2\u001b[39m], \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdate_after_tomorrow\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexing.py:818\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    817\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 818\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexing.py:1795\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[39m# align and set the values\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m \u001b[39mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1794\u001b[0m     \u001b[39m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1795\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[0;32m   1796\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1797\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexing.py:1879\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1876\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_single_column(ilocs[\u001b[39m0\u001b[39m], value, pi)\n\u001b[0;32m   1878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1880\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMust have equal len keys and value \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1881\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwhen setting with an iterable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1882\u001b[0m         )\n\u001b[0;32m   1884\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1885\u001b[0m \n\u001b[0;32m   1886\u001b[0m     \u001b[39m# scalar value\u001b[39;00m\n\u001b[0;32m   1887\u001b[0m     \u001b[39mfor\u001b[39;00m loc \u001b[39min\u001b[39;00m ilocs:\n",
      "\u001b[1;31mValueError\u001b[0m: Must have equal len keys and value when setting with an iterable"
     ]
    }
   ],
   "source": [
    "# y_predicted = model.predict(x_train)\n",
    "# y_predicted_transformed = np.squeeze(scaler.inverse_transform(y_predicted))\n",
    "\n",
    "# plt.figure(figsize=(16,10))\n",
    "# plt.plot(df['Close'].tolist())\n",
    "# plt.plot(y_predicted_transformed)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
